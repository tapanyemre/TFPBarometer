# -*- coding: utf-8 -*-
"""Filtrelemele & sentiment_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uwmVE_8Fk7lgmJITiTz11RZLNBz8WzVQ
"""

from google.colab import drive
drive.mount('/content/gdrive' , force_remount=True)
import sys

import pandas as pd
tweet_en_filtered = pd.read_csv('/content/gdrive/MyDrive/gmf/solutions/tweets/filtered_tweet_en_with_sentiment.csv', engine = 'python')
tweet_en_filtered.head()

len(tweet_en_filtered[tweet_en_filtered['username']=='akinunver'])

"""# English Filtreleme

"""

import pandas as pd
import os
os.chdir('/content/gdrive/MyDrive/gmf/') 
os.getcwd()
#pd.set_option("display.max_rows", None, "display.max_columns", None)

tweet_en = pd.read_csv('/content/gdrive/My Drive/gmf/tweet_en' , engine = 'python')#loading all the tweets,which are not filtred.

len(tweet_en)

tweet_en.head()

tweet_en.tweet = tweet_en.tweet.str.lower()
tweet_en.head()

#4:49
keywords_english = ['Turkey', 'US', 'United States', 'Turkish American', 'Turkish-American', 'strategic partnership', 'ally', 'crisis', 'Russia', 'Iran', 'Iranian' 'S400', 'S-400',
                    'Greece', 'Greek', 'authoritarian', 'authoritarianism', 'human rights', 'religious freedom', 'terrorism', 'Kurds', 'Kurdish', 'Egypt', 'Egyptian',
                    'China', 'Chinese', 'F35', 'F-35', 'sanctions', 'support', 'interest', 'Black Sea', 'Middle East', 'Mediterranean', 'Idlib', 'coup', 'GÃ¼len', 'Brunson',
                    'Cyprus', 'ErdoÄŸan', 'Ã‡avuÅŸoÄŸlu', 'security detail', 'PKK', 'YPG', 'PYD', 'Montreux', 'Inherent Resolve', 'anti-ISIL', 'anti-DAESH', 'DAESH', 'peace',
                    'migration', 'refugees', 'trade volume', 'partner', 'NATO', 'Southern Flank', 'block', 'troop surge', 'second largest army', 'Eastern Europe', 'Poland',
                    'axis', 'swap', 'IMF', 'hostage', 'ransom', 'anti-Americanism', 'enemy', 'friend', 'Islamist', 'competitive authoritarianism', 'defense industry', 'Ä°brahim KalÄ±n',
                    'Ã‡avuÅŸoÄŸlu', 'Akar', 'MFA', 'visit', 'Armenian genocide', 'appreciate', 'intelligence', 'support', 'counter terrorism', 'counter-terrorism', 'common interest',
                    'common threat', 'incursion', 'drone', 'UAV', 'sanctions', 'violation', 'EEZ', 'UNCLOS', 'Hamas', 'Muslim Brotherhood', 'Sudan', 'Deal of Century', 'Jerusalem',
                    'Arab NATO', 'Incirlik', 'nuclear bomb', 'migration crisis', 'insurgency', 'staunch', 'UAE', 'Saudi Arabia', 'Gulf', 'Qatar', 'Muslim Brotherhood', 'Hamas', 'Israel']
  

print(len(keywords_english))

# the first approach
keywords_english =[item.lower() for item in keywords_english]
#keywords_english

tweet_filtered = tweet_en[tweet_en['tweet'].str.contains('|'.join(keywords_english),na=False)]
tweet_filtered = tweet_filtered.reset_index(drop=True)
tweet_filtered.to_csv('/content/gdrive/My Drive/gmf/filtered_tweet_en',index=False)#saving filtered tweets to drive

import pandas as pd

tweet_filtered = pd.read_csv('/content/gdrive/My Drive/gmf/filtered_tweet_en') #loading to the google colab
tweet_filtered.shape

tweet_filtered.head()

topic_keywords_filtered = tweet_filtered[(tweet_filtered['tweet'].str.contains('turkey')) | (tweet_filtered['tweet'].str.contains('turkish'))
                                         
                                         & (tweet_filtered['tweet'].str.contains('american')) | (tweet_filtered['tweet'].str.contains(r'(?:\s|^)us(?:\s|$)'))
                                         ]
topic_keywords_filtered = topic_keywords_filtered.reset_index(drop=True)
len(topic_keywords_filtered)

topic_keywords_filtered['tweet'][0]

"""# English Frequency Analysis

"""

import nltk
import string
nltk.download('punkt')
nltk.download('stopwords')


user_defined_stop_words = ['especially' , 'getting' , 'according', 'without','years', 'take', 'use', 'point', 'since', 
                           'years', 'year', 'the' ,'https' , 'http' ,"''", "``" , "â€™" , "â€œ" , "n't" , "'d", 'com',
                           'status', 'pic', 'twitter', 'www', 'news' , 'org', 'ly' , 'bit' , 'bitly', 'html' , 'org',
                           'co' , 'know' , 'yes' , 'even', 'much' , 'really' , 'know' , 'well', 'much' , 'going', "really",
                           "always" , "sure", "read" , "things", "today" , "last", "new" , "read" , 'right', 'great' , 
                           'point',"one","would" ,"via" , "â€¦" ,"â€" ,  "..." ,"'s" , "time", "says" , "like", 
                           "good", "also", "get" , "said", "could", "say", "much","even", "many", "may" , "still" , 
                           "--" , "'m", "'re", 'ğŸ“¸', "want" , "another" , "piece", "let" , "made", "got","'ve" , 
                           'seems' , 'look' ,  "'ll" ,"rt" , "'â€˜'", "â€˜", "ca", "actually" , "'â€”'" , "â€”", 
                           'ever' , 'lot' , 'already' , "maybe", 'used', 'though' , '..', 'likely' , 'nothing' ,
                           "people" , "know" "right", "see" , "really" , "day" , "well", "much", "going" , "even" , 
                           "yes" , "way" , "never" , "go" , "something" , "thing" , "make" , "back"  , 'watch' , 
                           'youtube', 'one' , "the" , "to",  "da" ,"is" ,"for" ] 
i = nltk.corpus.stopwords.words('english')
j = list(string.punctuation) + user_defined_stop_words
stopwords = set(i).union(j) 
stopwords_en = list(stopwords)

#txt_en
#words_en =  nltk.tokenize.word_tokenize(deneme.tweet.str.replace(r'\|', ' ').str.cat(sep=' '))

#words_except_stop_dist_en = nltk.FreqDist(w for w in words_en if w not in stopwords_en)
#del words_en
#words_except_stop_dist_en

#words_en = nltk.tokenize.word_tokenize(deneme.tweet.str.replace(r'\|', ' ').str.cat(sep=' '))
#word_dist_en = nltk.FreqDist(words_en)
#del words_en
#words_except_stop_dist_en = nltk.FreqDist(w for w in words_en if w not in stopwords_en)

#txt_en
words_en =  nltk.tokenize.word_tokenize(tweet_filtered.tweet.str.replace(r'\|', ' ').str.cat(sep=' '))

#words_except_stop_dist_en = nltk.FreqDist(w for w in words_en if w not in stopwords_en)

####while the re-runnigh the code  will yield the full table of 200 words with frequencies

top_N = 200


print('All frequencies, excluding STOPWORDS:')
print('=' * 60)
rslt_200 = pd.DataFrame(words_except_stop_dist_en.most_common(top_N),
                    columns=['Word', 'Frequency']).set_index('Word')
pd.set_option('display.max_rows', 200)
print(rslt_200)
print('=' * 60)

rslt_200_en = list(rslt_200.index)
rslt_200_en

most_common_en = words_except_stop_dist_en.most_common()

len(most_common_en)

most_common_en[len(most_common_en)-100:]###en az gecen 100



"""# Turkce Filtreleme

The same procedures implied to English, filtering, cleaning and finding the frequency of the words
"""

import pandas as pd
tweet_tr = pd.read_csv('/content/gdrive/My Drive/gmf/solutions/tweets/tweet_tr.csv' , engine='python')
len(tweet_tr)

keywords_turkish = ['ABD', 'Amerika', 'TÃ¼rkiye', 'TÃ¼rk', 'Amerikan', 'PKK', 'PYD', 'S400', 'S-400', 'Rusya', 'BatÄ±', 'F35' , 'F-35' , 
                    'savaÅŸ uÃ§aÄŸÄ±', 'CAATSA', 'darbe',
                    'Pastor Brunson', 'Rahip Brunson', 'GÃ¼len', 'FETÃ–', 'mÃ¼ttefik', 'eksen', 'emperyalizm', 'destek', 'BOP ', 'BÃ¼yÃ¼k OrtadoÄŸu Projesi', 'terÃ¶r',
                    'terÃ¶rist', 'IÅÄ°D', 'DAEÅ', 'DEAÅ', 'Stratejik ortak', 'stratejik ortaklÄ±k', 'Ä°ncirlik', 'iÅŸbirliÄŸi', 'yardÄ±m', 'ticaret hacmi', '100 milyar Dolar',
                    'darbe', 'KÄ±brÄ±s', 'ÅŸantaj', 'tehdit', 'rehine', 'eÄŸit-donat', 'eÄŸit donat', 'Ã§ekilme', 'Suriye', 'Irak', 'CENTCOM', 'Pentagon', 'ErdoÄŸan', 'Obama',
                    'Trump', 'McGurk', 'Halkbank', 'Michael Flynn', 'DoÄŸu Akdeniz', 'Patriot', 'Ä°ran yaptÄ±rÄ±m', 'Dost', 'Dostluk', 'swap', 'NATO', 'NATO zirve(si)',
                    'BarÄ±ÅŸ PÄ±narÄ±', 'FÄ±rat KalkanÄ±', 'Zeytin DalÄ±', 'Bahar KalkanÄ±', 'Ziyaret', 'Kongre', 'Lobi', 'Graham', 'TasarÄ±', 'yaptÄ±rÄ±m', 'Bolton', 'Clinton',
                    'IMF', 'UluslararasÄ± Para Fonu', 'Kriz', 'istihbarat', 'savunma sanayii', 'Ã‡avuÅŸoÄŸlu', 'Akar', 'Esper', 'McMaster', 'LNG', 'Shale', 'petrol',
                    "Robert C. O'Brien", 'KRG', 'BÃ¶lgesel KÃ¼rt YÃ¶netimi', 'Ä°srail- YÃ¼zyÄ±lÄ±n PlanÄ±', 'BarÄ±ÅŸ planÄ±', 'Ä°srail', 'OrtadoÄŸu', 'KudÃ¼s bÃ¼yÃ¼kelÃ§ilik', 'Ä°slam',
                    'Evanjelik', 'insan haklarÄ±', 'Ã¶zgÃ¼rlÃ¼kler', 'medya', 'oyun', 'plan', 'Ã§Ä±kar', 'TÃ¼rkiye karÅŸÄ±tÄ±','dÃ¼ÅŸman', 'Ermeni soykÄ±rÄ±mÄ± iddiasÄ±', 'ihanet',
                     'UNCLOS', 'mÃ¼nhasÄ±r ekonomik bÃ¶lge', 'MEB', 'EEZ']

keywords_turkish =[item.lower() for item in keywords_turkish]

en_car = ['turkiye' , 'turk' , 'bati' , 'savas ucagi' , 'gulen' , 'feto' , 'muttefik' , 'buyuk ortadogu projesi' , 'teror',
'terrorist' , 'terorist' , 'isid' , 'daes' , 'deas' , 'isbirligi' , 'yardim' , 'kibris' , 'santaj' , 'cekilme' , 'irak' , 
'yardim' , 'dogu akdeniz' , 'iran yaptirim' , 'tasari' , 'bahar kalkani' , 'zeytin dali' , 'firat kalkani' , 'baris pinari',
'imf' , 'cavusoglu' , 'kudus buyukelcilik' , 'ortadogu', 'baris plani' , 'bolgesel kurt yonetimi' , 'insan haklari' , 'ozgurluk', 'ozgurlukler',
'cikar' , 'turkiye karsiti' , 'dusman' , 'ermeni soykirimi iddiasi' , 'soykirim' , 'munhasir ekonomik bolge']
len(en_car)

keywords_turkish = keywords_turkish + en_car

print(len(keywords_turkish))

tweet_filtered_tr = tweet_tr[tweet_tr['tweet'].str.contains('|'.join(keywords_turkish),na=False)]
tweet_filtered_tr = tweet_filtered_tr.reset_index(drop=True)
tweet_filtered_tr.to_csv('/content/gdrive/My Drive/gmf/solutions/tweets/filtered_tweet_tr.csv',index=False)

"""Ä°kinci filtreleme kelimeleri: TÃ¼rkiye, TÃ¼rk, Amerikan, ABD, Amerika"""

import pandas as pd
tweet_filtered_tr = pd.read_csv('/content/gdrive/My Drive/gmf/filtered_tweet_tr' , engine='python')

topic_keywords_filtered_tr = tweet_filtered_tr[(tweet_filtered_tr['tweet'].str.contains('tÃ¼rkiye')) | (tweet_filtered_tr['tweet'].str.contains('tÃ¼rk'))
                                         
                                         & (tweet_filtered_tr['tweet'].str.contains('abd')) | (tweet_filtered_tr['tweet'].str.contains('amerika'))
                                         ]
topic_keywords_filtered_tr = topic_keywords_filtered_tr.reset_index(drop=True)
len(topic_keywords_filtered_tr)

"""# Turkce Frequency Analysis"""

import re
tweet_filtered_tr.tweet = tweet_filtered_tr.tweet.apply(lambda x: re.sub(r"Ä°", "i",x))
tweet_filtered_tr.tweet = tweet_filtered_tr.tweet.str.lower()
tweet_filtered_tr.head()

import nltk
import string
nltk.download('punkt')
nltk.download('stopwords')
from nltk import sent_tokenize, word_tokenize
user_defined_stop_words_tr = [ 'â€','â€œ','son', 'ilk', '1' , '2' ,'3', 'arasÄ±nda' , 'fazla' , 'yeniden' , 'bunun', 'etmek', 'dedi' , 'buna' , 'bugÃ¼nkÃ¼',
 'bugÃ¼n', 'â€¦', 'â€™', '...', "''", '``', "ne" , "kadar" , "ve" , "iyi" , "ki" , "Ã§ok" , "ya" , "diye" , "gibi" , "com", 'Ã¶yle' , "Ã¶nce" , 
                              "twitter" , "pic" , "ile" , "en" ,"http" , "https" , "html" , "aracÄ±lÄ±ÄŸÄ±yla" , "iÃ§in",
                              "deÄŸil", "Ã§ok" , "daha"  , "ile" , "ama" , "ne"  , "gibi" ,  "her" , 'status' , "pic", "org", "id",
                              "aspx" , "ile" , "teÅŸekkÃ¼rler" , "bu" , "youtube" , "teÅŸekkÃ¼r" , "the" , "to",  "da" ,"is" ,"for",  "eyvallah",                            
                              'http' , 'bir' , 'var' , 'yok', 'olarak' , 'mi', 'ben', 'olan', 'olsun', 'bu' , 'de' , 'da' , 'ama', 
    'nin' , 'in' , 'bile' , 'bÃ¶yle' , 'olur' , 'gÃ¶re' , 'cok' , 'tek' , 'iki' , 'oldu' ,  'kendi', "ÅŸimdi", 'yeni', 'bugÃ¼n' , 'yazar', 'olun',
     'hocam' , 'be' , 'evet' ,  'sonra' , 'youtu',  'bence' ,'size' , 'sana' , 'zaman', "www", 'yazarlar' , 'haber' , 'via',
 'olduÄŸu','icin', 'bunu','ilgili','artÄ±k', 'aynÄ±' , 'a' , 'ancak' , 'ederim' , 'zaten' , 'olacak' , 'ediyor', 'gerek', 'bize',
 'yine' , 'tam' , 'baÅŸka' , 'benim' ,  'Ä±n' , 'olduÄŸunu' , 'â€˜' ,  'iÃ§inde' , 'sen' , 'olmak' , 'hala', 'demek' , 'bizim',
 'eden' , 'bana' , 'bi' ,  'Ã¼zerine' , 'tarafÄ±ndan', 'bi', 'olsa' , 'diyor' ,  'nÄ±n' , 'ye' , 'etti', 'geldi' , 'degil',
 'uzun' , 'bin' ,'oluyor' , 'ÅŸekilde' , 'e' , 'of' , 'el' , 'i' , 'diÄŸer',  'dair' , 'yerine' , 'geliyor' , 'olmasÄ±' , 'an',
 'pek' ,  'den' , 'yoksa', 'olmaz' , 'Ã¼zerinden' , 'onu' , 'beni' , 'seni' , 'te' , 'yi' ,'karsi' , 'karÅŸÄ±' , "'i" , "'Ä±" , "yÄ±" ,'e' , 'den' ,'dan' ,'tan',
 'ten' , 'watch' , 'tebrikler' , 'sadece' , 'and' ,  'biraz' , 'on', 'lazÄ±m' , 'tek' , 'sizin' , 'olabilir' , 'olmuÅŸ']
i = nltk.corpus.stopwords.words('turkish')
j = list(string.punctuation) + user_defined_stop_words_tr
stopwords_tr = set(i).union(j)
stopwords_tr = list(stopwords_tr)
type(stopwords_tr)

stopwords_tr

txt_tr = tweet_filtered_tr.tweet.str.lower().str.replace(r'\|', ' ').str.cat(sep=' ')
words_tr = nltk.tokenize.word_tokenize(txt_tr)
word_dist_tr = nltk.FreqDist(words_tr)
words_except_stop_dist_tr = nltk.FreqDist(w for w in words_tr if w not in stopwords_tr)

top_N = 200


print('All frequencies, excluding STOPWORDS:')
print('=' * 60)
rslt_200_tr = pd.DataFrame(words_except_stop_dist_tr.most_common(top_N),
                    columns=['Word', 'Frequency']).set_index('Word')
print(rslt_200_tr)
print('=' * 60)

#en az gecen 100 kelime
most_common_tr = words_except_stop_dist_tr.most_common() 

most_common_tr[(len(most_common_tr)-100):]

"""# English Sentiment"""

tweets_en =  []
for i in tweet_en['tweet']:
  tweets_en.append(i)

tweet_filtered.head()

tweet_filtered_list =  []
for i in tweet_filtered['tweet']:
  tweet_filtered_list.append(i)

type(tweets_en[0])

pip install vaderSentiment

###installing the vadersentiment library###



from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyser = SentimentIntensityAnalyzer()
def sentiment_analyzer_scores(sentence):
    score = analyser.polarity_scores(sentence)
    print("{:-<40} {}".format(sentence, str(score)))

#print(analyser.polarity_scores(tweet_en.tweet[87]))   
#print(analyser.polarity_scores(tweet_en.tweet[87])['compound'])

sentiment_score = []
for j in tweet_en.tweet:
  sentiment_score.append(analyser.polarity_scores(str(j)))

deneme = tweet_filtered.head()

sentiment_score_filtered = []
for j in range(len(deneme.tweet)):
  scores = analyser.polarity_scores(deneme.tweet[j])
  sentiment_score_filtered.append(scores['compound'])

sentiment_score_filtered

sentiment_score_filtered = []
for j in tweet_filtered.tweet:
  sentiment_score_filtered.append(analyser.polarity_scores(str(j)))

sentiment_score_filtered[0]['compound']

compound_filtr = []
for i in range(len(sentiment_score_filtered)):
  compound_filtr.append(sentiment_score_filtered[i]['compound'])

sentiment_score[0]['compound']

###there are various different sentiment scores, for our analyzes it is better to use compound score###

compound = []
for i in range(len(sentiment_score)):
  compound.append(sentiment_score[i]['compound'])

sentiment_score_filtered = []
for j in range(len(tweet_filtered.tweet)):
  scores = analyser.polarity_scores(tweet_filtered.tweet[j])
  sentiment_score_filtered.append(scores['compound'])

len(sentiment_score_filtered) == len(tweet_filtered)

from pandas import DataFrame

tweet_filtered['sentiment'] = DataFrame (sentiment_score_filtered,columns=['sentiment'])

tweet_filtered.head(n=3)

tweet_filtered.to_csv('/content/gdrive/My Drive/gmf/filtered_tweet_en_with_sentiment.csv',index=False)

tweet_en['sentiment'] = DataFrame (compound,columns=['sentiment'])

tweet['sentiment'] = DataFrame (compound,columns=['sentiment'])

tweet_filtered.head()

tweet_en.to_csv('/content/gdrive/My Drive/gmf/solutions/ en_tweet_no_filter_with_sentiment.csv')

tweet_en['Compound'] = DataFrame (compound,columns=['Compound'])
df = tweet_en.groupby(tweet_en.date).mean()
df.head()
tweet_en['date'] = pd.to_datetime(tweet_en['date'], errors='coerce')
tweet_en['date'] = sorted(tweet_en['date'])
tweet_en

df.head()

tweet_en['date'] = pd.to_datetime(tweet_en['date'], errors='coerce')
tweet_en['date'] = sorted(tweet_en['date'])
tweet_en

# Commented out IPython magic to ensure Python compatibility.
####this code illustrates the sentiment and save it to spesificed directory/location
import matplotlib.dates as mdates
import matplotlib.pyplot as plt

# %matplotlib inline
date = list(tweet_en.date)
compound = list(tweet_en.Compound)

plt.style.use('ggplot')
#9 hazirana kadar data var
fig, ax = plt.subplots(1, 1, figsize=(24, 6))
plt.plot(df.index, df.Compound)
#ax.xaxis.set_minor_locator(mdates.WeekdayLocator(byweekday=(0), interval=1))
# Set x-axis major ticks to weekly interval, on Mondays
#MonthLocator: locate months, e.g., 7 for july
#ax.xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=(0), interval=30))
ax.xaxis.set_major_locator(mdates.MonthLocator(interval=4))
# Format x-tick labels as 3-letter month name and day number
ax.xaxis.set_major_formatter(mdates.DateFormatter('%b/%Y'));
plt.xticks(rotation=45)
plt.title('Daily Sentiments of English Tweets')
plt.savefig('/content/gdrive/My Drive/css/daily_sentiment.png',dpi=600)#cozunurluk ve dosya adi
plt.show()

tweet_en.Compound

tweets_en_filtered =  []
for i in tweet_filtered['tweet']:
  tweets_en_filtered.append(i)

type(tweets_en_filtered[0])

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyser = SentimentIntensityAnalyzer()
def sentiment_analyzer_scores(sentence):
    score = analyser.polarity_scores(sentence)
    print("{:-<40} {}".format(sentence, str(score)))

print(analyser.polarity_scores(tweets_en_filtered[87]))   
print(analyser.polarity_scores(tweets_en_filtered[87])['compound'])

sentiment_score = []
for j in tweet_filtered.tweet:
  sentiment_score.append(analyser.polarity_scores(j))
#print(len(sentiment_score))

compound = []
for i in range(len(sentiment_score)):
  compound.append(sentiment_score[i]['compound'])

tweet_filtered['Compound'] = DataFrame (compound,columns=['Compound'])
df = tweet_filtered.groupby(tweet_filtered.date).mean()
df.head()
tweet_filtered['date'] = pd.to_datetime(tweet_filtered['date'], errors='coerce')
tweet_filtered['date'] = sorted(tweet_filtered['date'])
tweet_filtered

# Commented out IPython magic to ensure Python compatibility.
####this code illustrates the sentiment and save it to spesificed directory/location
import matplotlib.dates as mdates
import matplotlib.pyplot as plt

# %matplotlib inline
date = list(tweet_filtered.date)
compound = list(tweet_filtered.Compound)

plt.style.use('ggplot')
#9 hazirana kadar data var
fig, ax = plt.subplots(1, 1, figsize=(24, 6))
plt.plot(df.index, df.Compound)
#ax.xaxis.set_minor_locator(mdates.WeekdayLocator(byweekday=(0), interval=1))
# Set x-axis major ticks to weekly interval, on Mondays
#MonthLocator: locate months, e.g., 7 for july
#ax.xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=(0), interval=30))
ax.xaxis.set_major_locator(mdates.MonthLocator(interval=4))
# Format x-tick labels as 3-letter month name and day number
ax.xaxis.set_major_formatter(mdates.DateFormatter('%b/%Y'));
plt.xticks(rotation=45)
plt.title('Daily Sentiments of Filtered English Tweets')
plt.savefig('/content/gdrive/My Drive/gmf/english_filtered_daily_sentiment.png',dpi=900)#cozunurluk ve dosya adi
plt.show()

"""# Turkce Sentiment"""

import os
os.chdir('/content/gdrive/My Drive/gmf/')

!pip install --upgrade tensorflow==2.1
!pip3 install ktrain==0.18
!pip3 install transformers==3.0.2

### if this part gives error you should resttart the runtime for libraries to be installed

import ktrain
from ktrain import text
import pandas as pd
import re
import tensorflow

print(tensorflow.__version__)

from google_drive_downloader import GoogleDriveDownloader as gdd

gdd.download_file_from_google_drive(file_id='18Ev6LWZTcTRnS3cLrQJ3tGKD1cHS9bXr',
                                    dest_path='/content/gdrive/My Drive/gmf',
                                    unzip=True)

#gdd.download_file_from_google_drive(file_id='19DDNzy1uIk90MkYszOfl7ngJRR1k8M7T',
                                   #dest_path='/content/gdrive/My Drive/gmf',  # SALDIRGAN DÄ°L TESPÄ°TÄ°
                                    #unzip=True)

#gdd.download_file_from_google_drive(file_id='1XN348Ov8V6vjReKV5ZODelM3Kn_IuC-L',
                                    #dest_path='/content/gdrive/My Drive/gmf/', # TEYÄ°DE MUHTAÃ‡LIK TESPÄ°TÄ°
                                    ##unzip=True)

#### sentiment analizi paketinin yuklenmesi###

gdd.download_file_from_google_drive(file_id='1ooaswOvJlelulYt9meEhUndQQMgNgK2n',
                                    dest_path='./ft5m_sa.zip',# DUYGU ANALÄ°ZÄ° TESPÄ°TÄ°
                                    unzip=True)

import os
os.chdir('/content/gdrive/My Drive/gmf/')
os.getcwd()

#model_off = ktrain.load_predictor("ft5m_off")  # NOT, OFF

#model_cw = ktrain.load_predictor("ft5m_cw")     # 0, 1

model_sa = ktrain.load_predictor("ft5m_sa")#POS, NEG, NOTR

### FINE TUNE MODELLERI KULLANMAK Ä°Ã‡Ä°N Ã–N EÄÄ°TÄ°LMÄ°Å BERT MODELÄ°N HANGÄ°SÄ° OLDUÄU BELÄ°RTÄ°LMESÄ° GEREKÄ°YOR
#model_off.preproc.model_name = "berturk-social-5m"
#model_cw.preproc.model_name = "berturk-social-5m"
model_sa.preproc.model_name = "berturk-social-5m"

sentence = "bu cok kotu bir cumle"

verbose=0 
print(model_sa.predict(sentence))
print(type(model_sa.predict([sentence])))

import pandas as pd
filtered_tweet_tr_en_car = pd.read_csv('/content/gdrive/My Drive/gmf/solutions/tweets/filtered_tweet_tr.csv' , engine='python')

len(filtered_tweet_tr_en_car)

filtered_tweet_tr_en_car

filtered_tweet_tr_en_car['date'] =  pd.to_datetime(filtered_tweet_tr_en_car['date'], errors='coerce')

type(filtered_tweet_tr_en_car['date'])

import re
filtered_tweet_tr_en_car.tweet = filtered_tweet_tr_en_car.tweet.apply(lambda x: re.sub(r"Ä°", "i",x))
filtered_tweet_tr_en_car.tweet = filtered_tweet_tr_en_car.tweet.str.lower()

filtered_tweet_tr_en_car.head()

deneme = filtered_tweet_tr_en_car.tweet[:]
deneme

deneme = filtered_tweet_tr_en_car.tweet[:]
deneme
filtered_tweet_tr_en_car.tweet = filtered_tweet_tr_en_car.tweet.astype(str)
deneme = list(filtered_tweet_tr_en_car.tweet)

deneme

len(deneme) == len(rest)

###with the new data you should wait the result of this code a bit###
#deneme = filtered_tweet_tr_en_car.tweet[:]
#pd.DataFrame(model_sa.predict(deneme)).to_csv('rest_sentiment.csv')
sent_list = model_sa.predict(deneme)

len(sent_list) == len(rest)

sentiment = []
for i in sent_list:
  if i == 'NEG':
    sentiment.append(-1)
  elif i == 'POS':
    sentiment.append(1)
  elif i == 'NOTR': 
    sentiment.append(-1)
filtered_tweet_tr_en_car = filtered_tweet_tr_en_car.assign(sentiment = sentiment)

filtered_tweet_tr_en_car.to_csv('/content/gdrive/MyDrive/gmf/solutions/tweets/filtered_tweet_tr_with_sentiment.csv')



"""# Farkli meslek gruplarinin sentiment analizleri"""

# Commented out IPython magic to ensure Python compatibility.
def visualize(df1,df2):
  import matplotlib.dates as mdates
  import matplotlib.pyplot as plt

#   %matplotlib inline


  plt.style.use('ggplot')
  #9 hazirana kadar data var
  fig, ax = plt.subplots(1, 1, figsize=(24, 6))
  plt.plot(df1.index, df1.sentiment, label = 'gazeteci')
  plt.plot(df2.index, df2.sentiment , label = 'akademik')
  plt.legend(loc="upper right")
#ax.xaxis.set_minor_locator(mdates.WeekdayLocator(byweekday=(0), interval=1))
# Set x-axis major ticks to weekly interval, on Mondays
#MonthLocator: locate months, e.g., 7 for july
#ax.xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=(0), interval=30))
  ax.xaxis.set_major_locator(mdates.MonthLocator(interval=4))
# Format x-tick labels as 3-letter month name and day number
  ax.xaxis.set_major_formatter(mdates.DateFormatter('%b/%Y'));
  plt.xticks(rotation=45)
#plt.title('Daily Sentiments of Turkish Tweets')
  plt.savefig('tr_sentiment.png',dpi=600)
  plt.show()

account_info = pd.read_csv('accounts_info.csv')
account_info.head()

account_info.Meslek.unique()

"""Farkli meslek gruplarinin tweetlerinin ayri bir yerde toplanmasi"""

gazeteci= account_info[account_info['Meslek']=='Gazeteci']['Twitter Handle']
gazeteci = gazeteci.reset_index(drop=True)
gazeteci_list = list(gazeteci)
print(gazeteci_list[0])
gazeteci_tweet = tr_tweet[tr_tweet['username'].isin(gazeteci_list)]
gazeteci_tweet

academics= account_info[account_info['Meslek']=='Akademisyen/AraÅŸtÄ±rmacÄ±']['Twitter Handle']
academics = academics.reset_index(drop=True)
academics_list = list(academics)
print(academics_list[0])
academics_tweet = tr_tweet[tr_tweet['username'].isin(academics_list)]
academics_tweet

diplomat= account_info[account_info['Meslek']=='BÃ¼rokrat/Diplomat']['Twitter Handle']
diplomat = diplomat.reset_index(drop=True)
diplomat_list = list(diplomat)
print(diplomat_list[0])
diplomat_tweet = tr_tweet[tr_tweet['username'].isin(diplomat_list)]
diplomat_tweet

def profession_tweet(profession):#options are : 'Akademisyen/AraÅŸtÄ±rmacÄ±', 'Gazeteci', 'SiyasetÃ§i', 'Freelance', 'BÃ¼rokrat/Diplomat', 'DiÄŸer',
       'SiyasetÃ§i', 'Freelance', 'BÃ¼rokrat/Diplomat'
  df_ = account_info[account_info['Meslek']==profession]['Twitter Handle']
  df_ =df_.reset_index(drop=True)
  df_list = list(df_)
  print(df_list[0])
  df_tweet = tr_tweet[tr_tweet['username'].isin(df_list)]
  df_tweet.head()
  return df_tweet.groupby(df_tweet.date).mean()

del df2
df2 = academics_tweet.groupby(academics_tweet.date).mean()

del df1
df1 = gazeteci_tweet.groupby(gazeteci_tweet.date).mean()

del df3
df3 = diplomat_tweet.groupby(diplomat_tweet.date).mean()

"""if you instert he profession to the code below, it will give you the tweets of the spesific profession. Then you can use that dataframe(df) to visualize below"""

df4 = profession_tweet('instert one of the profession above')

"""you can create different data frames of different professIons"""

visualize(df1,df2)#while inserting some other df's you can obtain graphs for different professsio

"""# Miscalleneous"""

df_all.to_csv('filtered_tr_new_with_sentiment.csv' , index=False)
rest = filtered_tweet_tr_en_car[filtered_tweet_tr_en_car['date'] > '2020-05-13']

rest = filtered_tweet_tr_en_car[filtered_tweet_tr_en_car['date'] > '2020-08-06']

df = df_old[['date','username', 'tweet', 'sentiment']]
df

df_all = pd.concat([df_old,rest])
df_all

sentiment = pd.read_csv('/content/gdrive/MyDrive/gmf/rest_sentiment.csv')
sentiment

"""
df = df[0]
sentiment = []
for i in df:
  if i == 'NEG':
    sentiment.append(-1)
  elif i == 'POS':
    sentiment.append(1)
  elif i == 'NOTR': 
    sentiment.append(-1)
"""

filtered_tweet_tr_en_car =filtered_tweet_tr_en_car.assign(sentiment = sentiment)

filtered_tweet_tr_en_car.head()

import os
os.chdir('/content/gdrive/My Drive/gmf/')
os.getcwd()

##dataset which includes sentiment scores###
tr_tweet = pd.read_csv('/content/gdrive/My Drive/gmf/tr_filtered_with_sentiment.csv' , engine='python')

tr_tweet.head()

df = tr_tweet.groupby(tr_tweet.date).mean()
df

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.dates as mdates
import matplotlib.pyplot as plt

# %matplotlib inline


plt.style.use('ggplot')
#9 hazirana kadar data var
fig, ax = plt.subplots(1, 1, figsize=(24, 6))
plt.plot(df.index, df.sentiment)

#ax.xaxis.set_minor_locator(mdates.WeekdayLocator(byweekday=(0), interval=1))
# Set x-axis major ticks to weekly interval, on Mondays
#MonthLocator: locate months, e.g., 7 for july
#ax.xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=(0), interval=30))
ax.xaxis.set_major_locator(mdates.MonthLocator(interval=4))
# Format x-tick labels as 3-letter month name and day number
ax.xaxis.set_major_formatter(mdates.DateFormatter('%b/%Y'));
plt.xticks(rotation=45)
plt.title('Daily Sentiments of Filtered Turkish Tweets')#baslik ayarlama title
plt.savefig('/content/gdrive/My Drive/gmf/sentiment_graphs/tr_filtered_sentiment.png',dpi=600)
plt.show()

import os
os.chdir('/content/gdrive/My Drive/gmf/')
### if this part gives error you should resttart the runtime for libraries to be installed

import ktrain
from ktrain import text
import pandas as pd
import re
import tensorflow
model_sa = ktrain.load_predictor("ft5m_sa")#POS, NEG, NOTR
model_sa.preproc.model_name = "berturk-social-5m"
sentence = "bu cok kotu bir cumle"

verbose=0 
print(model_sa.predict(sentence))
print(type(model_sa.predict([sentence])))

import pandas as pd
tweet_tr_no_filter = pd.read_csv('/content/gdrive/My Drive/gmf/tweet_tr' , engine='python')
tweet_tr_no_filter.tweet = tweet_tr_no_filter.tweet.str.replace("Ä°", "i", regex=True)
tweet_tr_no_filter.tweet = tweet_tr_no_filter.tweet.str.lower()

#tweet_tr_no_filter.tweet  = str(tweet_tr_no_filter.tweet)
#tweet_tr_no_filter.tweet = tweet_tr_no_filter.tweet[:].apply(lambda x: re.sub(r"Ä°", "i",x))
#tweet_tr_no_filter.tweet  = tweet_tr_no_filter.tweet.str.lower()
#tweet_tr_no_filter.tweet = tweet_tr_no_filter.tweet.str.replace("Ä°", "i", regex=True)

tweet_tr_no_filter.tweet = tweet_tr_no_filter.tweet.astype(str)
deneme = list(tweet_tr_no_filter.tweet)
#part1 = deneme[0:100000]
#part2 = deneme[100001:250000]
#part3 = deneme[800001:1250000]
#part4 = deneme[1250000:]

len

import os
os.chdir('/content/gdrive/My Drive/gmf/')
import pandas as pd
data = pd.read_csv('tr_final_All_sentiment.csv', header= None)
data

for i in data[0].head():
  print(i)

sent_tr_all = []
for i in data[0]:
  if i == 'POS':
    sent_tr_all.append(1)
  elif i == 'NEG':
    sent_tr_all.append(-1)
  else:
    sent_tr_all.append(0)
sent_tr_all[0:5]

len(sent_tr_all)

tweet_tr_no_filter = tweet_tr_no_filter.assign( sentiment = sent_tr_all )

tweet_tr_no_filter

tweet_tr_no_filter.to_csv('/content/gdrive/My Drive/gmf/tweet_tr_with_sentiment')

i = 0
k= 0
for k in range(len(data),1620936, 100000): 
  i = i+1
  print(i)
  temp = deneme[k:k+100000]
  duygu = model_sa.predict(temp)
  df = pd.DataFrame(duygu)
  df.to_csv('tr_final_All_sentiment.csv', mode='a', header=False , index= False)
  print(k+100000)

temp = deneme[1700000:]
duygu = model_sa.predict(temp)
df = pd.DataFrame(duygu)
df.to_csv('tr_final_All_sentiment.csv', mode='a', header=False , index= False)

tweet_tr_no_filter['date'] = pd.to_datetime(tweet_tr_no_filter['date'], errors='coerce')

df = tweet_tr_no_filter.groupby(tweet_tr_no_filter.date).mean()
df

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.dates as mdates
import matplotlib.pyplot as plt

# %matplotlib inline


plt.style.use('ggplot')
#9 hazirana kadar data var
fig, ax = plt.subplots(1, 1, figsize=(24, 6))
plt.plot(df.index, df.sentiment)

#ax.xaxis.set_minor_locator(mdates.WeekdayLocator(byweekday=(0), interval=1))
# Set x-axis major ticks to weekly interval, on Mondays
#MonthLocator: locate months, e.g., 7 for july
#ax.xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=(0), interval=30))
ax.xaxis.set_major_locator(mdates.MonthLocator(interval=4))
# Format x-tick labels as 3-letter month name and day number
ax.xaxis.set_major_formatter(mdates.DateFormatter('%b/%Y'));
plt.xticks(rotation=45)
plt.title('Daily Sentiments of All Turkish Tweets')#baslik ayarlama title
plt.savefig('tr_no_filter_sentiment.png',dpi=600)
plt.show()